## Custom CNN ‚Äì Generalizability Testing
A deep learning‚Äìbased project to evaluate the generalization capability of a Custom Convolutional Neural Network (CNN) for cotton leaf disease detection.
The project focuses on training a CNN model from scratch, performing rigorous testing on a second unseen dataset, and visualizing model explainability using Grad-CAM.
________________________________________
## üìå Features
‚Ä¢	Designed a Custom CNN architecture from scratch using PyTorch
‚Ä¢	Generalizability testing performed on a second dataset to evaluate robustness
‚Ä¢	Applied data augmentation techniques for better performance and reduced overfitting
‚Ä¢	Implemented stratified train-validation-test split (70:15:15)
‚Ä¢	Integrated Grad-CAM visualization for model interpretability
‚Ä¢	Analyzed performance using accuracy, precision, recall, F1-score, and confusion matrix
‚Ä¢	Optimized training using early stopping and checkpointing
________________________________________
## üìÇ Dataset
Primary Dataset (Training & Validation)
‚Ä¢	Cotton leaf disease dataset with 4 classes:
o	Bacterial Blight
o	Healthy
o	Curl Virus
o	Fussarium Wilt
Secondary Dataset (Generalizability Testing)
‚Ä¢	Used an unseen dataset to test the robustness of the trained model
Dataset Link: Cotton Leaf Disease Dataset
________________________________________
## üõ†Ô∏è Tools & Libraries Used
‚Ä¢	Programming Language: Python
‚Ä¢	Deep Learning Frameworks: PyTorch, Torchvision
‚Ä¢	Explainability Tools: Grad-CAM, Torchinfo
‚Ä¢	Visualization Libraries: Matplotlib, Seaborn
‚Ä¢	Data Handling: Pandas, NumPy
‚Ä¢	Model Evaluation: Scikit-learn
‚Ä¢	Others: Pillow, OpenCV, tqdm, Jupyter Notebook, Kaggle GPU
________________________________________
## üß† Model Architecture Overview
‚Ä¢	3 Convolutional Layers with Batch Normalization and ReLU Activation
‚Ä¢	MaxPooling & AveragePooling layers for feature extraction
‚Ä¢	Fully Connected Layers with Dropout to reduce overfitting
‚Ä¢	Softmax activation for 4-class classification
________________________________________
## ‚öôÔ∏è Training Configuration
‚Ä¢	Batch Size: 32 (Train), 8 (Validation/Test)
‚Ä¢	Image Size: 128√ó128
‚Ä¢	Optimizer: Adam
‚Ä¢	Learning Rate: 0.001
‚Ä¢	Loss Function: Cross-Entropy with label smoothing
‚Ä¢	Epochs: 500
‚Ä¢	Early Stopping: Patience = 200
‚Ä¢	Hardware Used: Kaggle Tesla P100 GPU
________________________________________
## üìä Training Results
‚Ä¢	Best Training Accuracy: 96.4%
‚Ä¢	Best Validation Accuracy: 97.7%
‚Ä¢	Training Loss: 0.6459
‚Ä¢	Validation Loss: 0.533
‚Ä¢	Generalizability testing on unseen dataset achieved high accuracy and robust predictions
________________________________________
## ‚úÖ Evaluation Metrics
‚Ä¢	Overall Test Accuracy: 97.8%
‚Ä¢	High precision, recall, and F1-score across all 4 classes
‚Ä¢	Confusion matrix visualization for better error analysis
________________________________________
## üîç Grad-CAM Visualization
‚Ä¢	Integrated Grad-CAM to visualize model attention regions
‚Ä¢	Helps understand why the model predicts a particular class
‚Ä¢	Enhances transparency and explainability of deep learning models
________________________________________

## üîÆ Future Enhancements
‚Ä¢	Compare performance with ResNet50, MobileNetV2, and VGG16
‚Ä¢	Deploy the model as a Flask / FastAPI web app
‚Ä¢	Test on real-world field images for better applicability
‚Ä¢	Enhance generalizability using larger and diverse datasets


